{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are testing a wine qualitybased on their acidity residual concentration alchohol content using random forest algorithm in machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in c:\\users\\asus\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\asus\\anaconda3\\lib\\site-packages (from sklearn) (0.21.3)\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.16.5)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.3.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (0.13.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imorting libraries\n",
    "import numpy as np # for array\n",
    "import pandas as pd # for array manipulation functions\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split # library for importing functions for ml here it will split input data to train and test\n",
    "from sklearn import preprocessing # for utilities like scaling wrangling transforming data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next is to imprt families of models.Family of models are broad type of models like random forest svm regression models etc .within each family of models we get actual model after fit and tune parameters to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we are imorting a family of model is randomforest model\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing tools for cross validation\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTING MATRICES FOR EVALUATING MODEL PERFORMANCE ERROR CORRECTION\n",
    "from sklearn.metrics import mean_squared_error,r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#importing  module for our model to persist in future alternative to python pickle package\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to import dataset\n",
    "data = pd.read_csv(\"winequality-red.csv\",sep=\";\") # sep=\";\" wich seperate csv to order.(read csv with semicolon seperator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5) #to print 1st 5rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RangeIndex(start=0, stop=1599, step=1), Index(['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n",
      "       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n",
      "       'pH', 'sulphates', 'alcohol', 'quality'],\n",
      "      dtype='object')]\n",
      "2\n",
      "(1599, 12)\n",
      "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
      "count    1599.000000       1599.000000  1599.000000     1599.000000   \n",
      "mean        8.319637          0.527821     0.270976        2.538806   \n",
      "std         1.741096          0.179060     0.194801        1.409928   \n",
      "min         4.600000          0.120000     0.000000        0.900000   \n",
      "25%         7.100000          0.390000     0.090000        1.900000   \n",
      "50%         7.900000          0.520000     0.260000        2.200000   \n",
      "75%         9.200000          0.640000     0.420000        2.600000   \n",
      "max        15.900000          1.580000     1.000000       15.500000   \n",
      "\n",
      "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
      "count  1599.000000          1599.000000           1599.000000  1599.000000   \n",
      "mean      0.087467            15.874922             46.467792     0.996747   \n",
      "std       0.047065            10.460157             32.895324     0.001887   \n",
      "min       0.012000             1.000000              6.000000     0.990070   \n",
      "25%       0.070000             7.000000             22.000000     0.995600   \n",
      "50%       0.079000            14.000000             38.000000     0.996750   \n",
      "75%       0.090000            21.000000             62.000000     0.997835   \n",
      "max       0.611000            72.000000            289.000000     1.003690   \n",
      "\n",
      "                pH    sulphates      alcohol      quality  \n",
      "count  1599.000000  1599.000000  1599.000000  1599.000000  \n",
      "mean      3.311113     0.658149    10.422983     5.636023  \n",
      "std       0.154386     0.169507     1.065668     0.807569  \n",
      "min       2.740000     0.330000     8.400000     3.000000  \n",
      "25%       3.210000     0.550000     9.500000     5.000000  \n",
      "50%       3.310000     0.620000    10.200000     6.000000  \n",
      "75%       3.400000     0.730000    11.100000     6.000000  \n",
      "max       4.010000     2.000000    14.900000     8.000000  \n"
     ]
    }
   ],
   "source": [
    "#pd operations\n",
    "print(data.axes) # gives no of elements start stop dtype\n",
    "print(data.ndim) # gives dimension of array\n",
    "print(data.shape) # gives no.of rows and column\n",
    "print(data.describe())# give detailed summary of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data to training and testing sets\n",
    "#seperating target(y) from input(x)\n",
    "y= data.quality #quality is our target\n",
    "X=data.drop('quality',axis=1) #we remove quality colum from our dataset.this is the input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data to training and testing sets\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state =123,stratify =y)\n",
    "#test data 20% and random state to reprodue exact data and stratify sample ensures your taining data looks similar to test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "declaring data for preprocessing\n",
    "Standardization is an important technique that is mostly performed as a pre-processing step before many Machine Learning models, to standardize the range of features of input data set.Standardization comes into picture when features of input data set have large differences between their ranges, or simply when they are measured in different measurement units (e.g., Pounds, Meters, Miles â€¦ etc).These differences in the ranges of initial features causes trouble to many machine learning modelsSo, to prevent this problem, transforming features to comparable scales using standardization is the solution.and can be done by subtracting the mean and dividing by the standard deviation for each value of each feature.Once the standardization is done, all the features will have a mean of zero, a standard deviation of one, and thus, the same scale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to perform standardization\n",
    "pipeline = make_pipeline(preprocessing.StandardScaler(),RandomForestRegressor(n_estimators=100))\n",
    "#here a modeling pipeline first transforms data using standardscaler()and then fit it using randomforestrege=ressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "declaring hyperparametres to tune\n",
    "In machine learning, a hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are derived via training.\n",
    "\n",
    "Hyperparameters can be classified as model hyperparameters, that cannot be inferred while fitting the machine to the training set because they refer to the model selection task, or algorithm hyperparameters, that in principle have no influence on the performance of the model but affect the speed and quality of the learning process. An example of a model hyperparameter is the topology and size of a neural network. Examples of algorithm hyperparameters are learning rate and mini-batch size.the model parameters specify how to transform the input data into the desired output, the hyperparameters define how our model is actually structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare hyperparametres to tune\n",
    "hyperparameters ={ 'randomforestregressor__max_features' : ['auto', 'sqrt', 'log2'],'randomforestregressor__max_depth': [None, 5, 3, 1]}\n",
    "#where keys are the hyperparameter names and values are lists of settings to try."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating a machine learning model, you'll be presented with design choices as to how to define your model architecture. Often times, we don't immediately know what the optimal model architecture should be for a given model, and thus we'd like to be able to explore a range of possibilities. In true machine learning fashion, we'll ideally ask the machine to perform this exploration and select the optimal model architecture automatically. Parameters which define the model architecture are referred to as hyperparameters and thus this process of searching for the ideal model architecture is referred to as hyperparameter tuning.\n",
    "\n",
    "Cross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data. That is, to use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model.\n",
    "Practically, that \"method\" is simply a set of hyperparameters in this context.\n",
    "\n",
    "These are the steps for CV:\n",
    "\n",
    "Split your data into k equal parts, or \"folds\" (typically k=10).\n",
    "Train your model on k-1 folds (e.g. the first 9 folds).\n",
    "Evaluate it on the remaining \"hold-out\" fold (e.g. the 10th fold).\n",
    "Perform steps (2) and (3) k times, each time holding out a different fold.\n",
    "Aggregate the performance across all k folds. This is your performance metric.\n",
    "Using only your training set, you can use CV to evaluate different hyperparameters and estimate their effectiveness.\n",
    "\n",
    "This allows you to keep your test set \"untainted\" and save it for a true hold-out evaluation when you're finally ready to select a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best practice when performing CV is to include your data preprocessing steps inside the cross-validation loop. This prevents accidentally tainting your training folds with influential data from your test fold.\n",
    "\n",
    "Here's how the CV pipeline looks after including preprocessing steps:\n",
    "\n",
    "Split your data into k equal parts, or \"folds\" (typically k=10).\n",
    "Preprocess k-1 training folds.\n",
    "Train your model on the same k-1 folds.\n",
    "Preprocess the hold-out fold using the same transformations from step (2).\n",
    "Evaluate your model on the same hold-out fold.\n",
    "Perform steps (2) - (5) k times, each time holding out a different fold.\n",
    "Aggregate the performance across all k folds. This is your performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('standardscaler',\n",
       "                                        StandardScaler(copy=True,\n",
       "                                                       with_mean=True,\n",
       "                                                       with_std=True)),\n",
       "                                       ('randomforestregressor',\n",
       "                                        RandomForestRegressor(bootstrap=True,\n",
       "                                                              criterion='mse',\n",
       "                                                              max_depth=None,\n",
       "                                                              max_features='auto',\n",
       "                                                              max_leaf_nodes=None,\n",
       "                                                              min_impurity_decrease=0.0,\n",
       "                                                              min_impurity_split=None,\n",
       "                                                              min_...\n",
       "                                                              min_weight_fraction_leaf=0.0,\n",
       "                                                              n_estimators=100,\n",
       "                                                              n_jobs=None,\n",
       "                                                              oob_score=False,\n",
       "                                                              random_state=None,\n",
       "                                                              verbose=0,\n",
       "                                                              warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'randomforestregressor__max_depth': [None, 5, 3, 1],\n",
       "                         'randomforestregressor__max_features': ['auto', 'sqrt',\n",
       "                                                                 'log2']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(pipeline, hyperparameters, cv=10) #creating 10 folds\n",
    " \n",
    "# Fit and tune model\n",
    "clf.fit(X_train, y_train)\n",
    "#GridSearchCV essentially performs cross-validation across the entire \"grid\" (all possible permutations) of hyperparameters.\n",
    "\n",
    "#It takes in your model (in this case, we're using a model pipeline), the hyperparameters you want to tune, and the number of folds to create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'randomforestregressor__max_depth': None, 'randomforestregressor__max_features': 'sqrt'}\n"
     ]
    }
   ],
   "source": [
    "print (clf.best_params_)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#REFIT ON THE ENTIRE TRAININGMODEL\n",
    "#GridSearchCV from sklearn will automatically refit the model with the best set of hyperparameters using the entire training set.\n",
    "print (clf.refit)\n",
    "#simply use the  clf object as your model when applying it to other sets of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVALUATE MODELPIPELINE ON TEST DATA\n",
    "y_pred =clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46961438343725415\n",
      "0.34224375\n"
     ]
    }
   ],
   "source": [
    "#we can use matrices for evaluating performance of our model\n",
    "print(r2_score(y_test,y_pred))\n",
    "print(mean_squared_error(y_test,y_pred)) #it compares with the actual test data and predictrd test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rf_regressor.pkl']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to save model for future use\n",
    "joblib.dump(clf, 'rf_regressor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to load the file use this\n",
    "clf2 = joblib.load('rf_regressor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.42, 5.77, 4.98, 5.53, 6.43, 5.52, 5.  , 4.66, 5.02, 6.04, 5.23,\n",
       "       5.78, 5.82, 5.1 , 5.74, 5.62, 6.51, 5.68, 5.59, 6.98, 5.56, 5.7 ,\n",
       "       5.08, 6.05, 5.92, 5.07, 5.4 , 5.12, 5.93, 5.93, 5.93, 6.52, 5.96,\n",
       "       5.02, 5.01, 5.97, 5.09, 6.11, 5.09, 5.87, 4.82, 5.95, 6.76, 5.19,\n",
       "       6.15, 5.4 , 5.55, 5.56, 5.1 , 6.47, 6.05, 5.14, 5.79, 5.22, 5.61,\n",
       "       5.65, 5.42, 5.38, 4.95, 5.27, 5.26, 5.1 , 5.08, 5.85, 5.99, 5.28,\n",
       "       6.37, 5.05, 5.13, 6.62, 5.65, 5.84, 5.11, 5.01, 5.33, 5.92, 5.35,\n",
       "       5.16, 5.19, 5.23, 6.39, 5.63, 6.28, 6.45, 5.15, 6.06, 6.36, 6.24,\n",
       "       5.77, 5.85, 5.88, 5.41, 6.35, 5.59, 5.64, 5.83, 6.76, 6.75, 5.56,\n",
       "       6.7 , 5.02, 5.33, 5.09, 6.4 , 5.03, 4.82, 5.69, 4.99, 5.56, 5.94,\n",
       "       5.76, 5.65, 5.99, 5.37, 5.32, 5.32, 5.92, 5.1 , 4.91, 5.88, 5.88,\n",
       "       5.07, 5.74, 6.15, 5.23, 5.41, 5.27, 6.09, 5.47, 5.48, 5.85, 6.3 ,\n",
       "       5.22, 5.29, 5.07, 6.54, 5.02, 5.14, 6.55, 5.43, 5.2 , 5.08, 5.59,\n",
       "       6.04, 5.39, 5.39, 5.08, 6.52, 5.79, 5.12, 5.54, 5.14, 4.99, 4.99,\n",
       "       5.29, 5.95, 5.41, 5.73, 5.71, 5.24, 5.58, 5.23, 5.29, 6.  , 5.05,\n",
       "       5.91, 5.12, 5.47, 5.48, 5.08, 6.11, 4.97, 5.57, 5.04, 5.62, 5.46,\n",
       "       5.07, 5.26, 5.6 , 5.05, 6.03, 5.55, 4.96, 4.93, 5.14, 6.14, 5.25,\n",
       "       5.64, 5.33, 4.81, 5.38, 6.52, 5.83, 5.92, 5.42, 5.13, 5.33, 5.06,\n",
       "       6.34, 4.66, 6.29, 5.09, 5.26, 5.28, 6.84, 6.02, 5.22, 5.23, 5.41,\n",
       "       5.83, 5.85, 5.99, 5.91, 6.24, 5.77, 5.96, 5.32, 5.29, 5.57, 5.2 ,\n",
       "       5.27, 6.01, 6.13, 5.43, 5.93, 5.86, 5.45, 6.18, 5.41, 5.9 , 5.47,\n",
       "       5.54, 6.38, 5.78, 4.91, 4.54, 6.94, 6.51, 6.35, 5.38, 5.33, 5.51,\n",
       "       5.55, 6.32, 5.97, 5.23, 5.14, 5.28, 5.22, 6.39, 5.16, 5.01, 5.34,\n",
       "       5.19, 5.87, 6.35, 5.81, 5.35, 5.47, 6.45, 5.47, 5.96, 5.36, 5.32,\n",
       "       5.64, 5.99, 5.82, 5.64, 5.49, 5.07, 5.79, 5.54, 6.67, 6.09, 5.65,\n",
       "       5.07, 5.91, 6.62, 6.02, 5.51, 5.58, 5.25, 5.24, 6.08, 6.85, 5.31,\n",
       "       6.44, 5.72, 5.43, 5.44, 5.58, 5.21, 5.2 , 6.27, 5.77, 5.95, 5.78,\n",
       "       5.92, 5.39, 5.64, 5.65, 6.15, 5.52, 6.82, 6.81, 5.86, 6.21, 5.06,\n",
       "       5.27, 5.97, 5.34, 5.38, 6.03, 6.6 , 6.52, 5.17, 5.67, 5.7 , 6.1 ,\n",
       "       5.55])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to predict\n",
    "clf2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
